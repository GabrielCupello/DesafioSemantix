{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Respostas e código para o desafio Semantix\n",
    "## Candidado Gabriel da Costa Cupello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Qual o objetivo do comando cache em Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R: O uso do comando cache permite salver na memória resultados parciais do processamento para serem utilizados nas próximas etapas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R: Porque o Spark utiliza processamento em memória, enquanto o MapReduce utiliza o disco. Como os processos de leitura e escrita (IOs) no disco são mais demorados em relação ao acesso dos dados em memória, o código escrito em Spark torna-se mais rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qual é a função do SparkContext?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R: O SparkContext representa a conexão com o Cluster Spark, permitindo que uma aplicação acesse o Cluster. Ele ainda é utilizado conmo intermediar podendo acessar RDDs, cancelar um job, alterar configurações, entre outras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explique com suas palavras o que é Resilient Distributed Datasets (RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R: É o elemento fundamental do Spark, sendo a maneira como os dados são processados. São um conjunto de dados imutáveis e armazenados em diferentes nós em um cluster. Suportam dois tipos de operação: transformação e ação. Na transformação realizam uma determinada operação com os dados como por exemplo, filtro e retornam um novo RDD com o resultado. Na ação realizam uma operação como, por exemplo, count e retornam o resultado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupByKey é menos eficiente que reduceByKey em grandes dataset. Por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R: Porque o reduceByKey realiza primeiro a redução em cada partição onde há dados para depois realizar a comuniação entre as outras partições via rede, enquanto que o groupByKey realiza a comunição via rede para realizar a redução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explique o que o código Scala abaixo faz.\n",
    "val textFile = sc.textFile(\"hdfs://...\")\n",
    "\n",
    "val counts = textFile.flatMap(line => line.split(\" \"))\n",
    "\n",
    "    .map(word => (word, 1))\n",
    "    \n",
    "    .reduceByKey(_ + _)\n",
    "    \n",
    "counts.saveAsTextFile(\"hdfs://...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R: Realiza a leitura de um arquivo de texto, em seguida quebra as palavras através dos espaços em branco. A partir disso, realiza cria uma tupla para cada palavra com o número 1, para a seguir realizar a redução, ou seja, realizando a contagem de cada palavra igual no texto do arquivo. Dessa forma, o algoritmo realiza a contagem de cada palavra no arquivo e salva o resultado em um arquivo de texto no HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código e respostas do desafio de programação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciar sessao\n",
    "spark = SparkSession.builder.appName(\"Desafio\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245                                 |\n",
      "|unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985                      |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085   |\n",
      "|burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0               |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179|\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# leitura dos arquivos\n",
    "df = spark.read.text(\"files/*\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3461613, 1)\n"
     ]
    }
   ],
   "source": [
    "# verificar tamanho de linhas e colunas\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir expressões regulares para separar os campos\n",
    "host_pattern = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "ts_pattern = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "method_uri_protocol_pattern = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "status_pattern = r'\\s(\\d{3})\\s'\n",
    "content_size_pattern = r'\\s(\\d+)$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+---------+-------+-----+\n",
      "|                host|                data|metodo|                 URL|protocolo|retorno|bytes|\n",
      "+--------------------+--------------------+------+--------------------+---------+-------+-----+\n",
      "|        199.72.81.55|01/Jul/1995:00:00...|   GET|    /history/apollo/| HTTP/1.0|    200| 6245|\n",
      "|unicomp6.unicomp.net|01/Jul/1995:00:00...|   GET| /shuttle/countdown/| HTTP/1.0|    200| 3985|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|   GET|/shuttle/missions...| HTTP/1.0|    200| 4085|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|   GET|/shuttle/countdow...| HTTP/1.0|    304|    0|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|   GET|/shuttle/missions...| HTTP/1.0|    200| 4179|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|   GET|/images/NASA-logo...| HTTP/1.0|    304|    0|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|   GET|/shuttle/countdow...| HTTP/1.0|    200|    0|\n",
      "|     205.212.115.106|01/Jul/1995:00:00...|   GET|/shuttle/countdow...| HTTP/1.0|    200| 3985|\n",
      "|         d104.aa.net|01/Jul/1995:00:00...|   GET| /shuttle/countdown/| HTTP/1.0|    200| 3985|\n",
      "|      129.94.144.152|01/Jul/1995:00:00...|   GET|                   /| HTTP/1.0|    200| 7074|\n",
      "+--------------------+--------------------+------+--------------------+---------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "(3461613, 7)\n"
     ]
    }
   ],
   "source": [
    "# aplicando as expressões regulares e separando os valores nos respectivos campos\n",
    "logs_df = df.select(regexp_extract('value', host_pattern, 1).alias('host'),\n",
    "                         regexp_extract('value', ts_pattern, 1).alias('data'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 1).alias('metodo'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 2).alias('URL'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 3).alias('protocolo'),\n",
    "                         regexp_extract('value', status_pattern, 1).cast('integer').alias('retorno'),\n",
    "                         regexp_extract('value', content_size_pattern, 1).cast('integer').alias('bytes'))\n",
    "logs_df.show(10, truncate=True)\n",
    "print((logs_df.count(), len(logs_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando view para poder utilizar a funcionalidade de banco de dados\n",
    "logs_df.createOrReplaceTempView(\"nasa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 1 - Número de hosts únicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT host)|\n",
      "+--------------------+\n",
      "|137933              |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_results = spark.sql(\"SELECT count(distinct(host)) from nasa\")\n",
    "sql_results.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 2 - O total de erros 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   20899|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_results = spark.sql(\"SELECT count(1) from nasa where retorno = '404'\")\n",
    "sql_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 3 - Os 5 URLs que mais causaram erro 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+--------+\n",
      "|URL                                         |count(1)|\n",
      "+--------------------------------------------+--------+\n",
      "|/pub/winvn/readme.txt                       |2004    |\n",
      "|/pub/winvn/release.txt                      |1732    |\n",
      "|/shuttle/missions/STS-69/mission-STS-69.html|683     |\n",
      "|/shuttle/missions/sts-68/ksc-upclose.gif    |428     |\n",
      "|/history/apollo/a-001/a-001-patch-small.gif |384     |\n",
      "+--------------------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_results = spark.sql(\"SELECT URL, count(1) from nasa where retorno = '404' group by URL order by 2 desc\")\n",
    "sql_results.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 4 - Quantidade de erros 404 por dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------+\n",
      "|substring(data, 1, 11)|count(1)|\n",
      "+----------------------+--------+\n",
      "|01/Aug/1995           |243     |\n",
      "|01/Jul/1995           |316     |\n",
      "|02/Jul/1995           |291     |\n",
      "|03/Aug/1995           |304     |\n",
      "|03/Jul/1995           |474     |\n",
      "|04/Aug/1995           |346     |\n",
      "|04/Jul/1995           |359     |\n",
      "|05/Aug/1995           |236     |\n",
      "|05/Jul/1995           |497     |\n",
      "|06/Aug/1995           |373     |\n",
      "|06/Jul/1995           |640     |\n",
      "|07/Aug/1995           |537     |\n",
      "|07/Jul/1995           |570     |\n",
      "|08/Aug/1995           |391     |\n",
      "|08/Jul/1995           |300     |\n",
      "|09/Aug/1995           |279     |\n",
      "|09/Jul/1995           |348     |\n",
      "|10/Aug/1995           |315     |\n",
      "|10/Jul/1995           |398     |\n",
      "|11/Aug/1995           |263     |\n",
      "|11/Jul/1995           |471     |\n",
      "|12/Aug/1995           |196     |\n",
      "|12/Jul/1995           |471     |\n",
      "|13/Aug/1995           |216     |\n",
      "|13/Jul/1995           |532     |\n",
      "|14/Aug/1995           |287     |\n",
      "|14/Jul/1995           |413     |\n",
      "|15/Aug/1995           |327     |\n",
      "|15/Jul/1995           |254     |\n",
      "|16/Aug/1995           |259     |\n",
      "|16/Jul/1995           |257     |\n",
      "|17/Aug/1995           |271     |\n",
      "|17/Jul/1995           |406     |\n",
      "|18/Aug/1995           |256     |\n",
      "|18/Jul/1995           |465     |\n",
      "|19/Aug/1995           |209     |\n",
      "|19/Jul/1995           |639     |\n",
      "|20/Aug/1995           |312     |\n",
      "|20/Jul/1995           |428     |\n",
      "|21/Aug/1995           |305     |\n",
      "|21/Jul/1995           |334     |\n",
      "|22/Aug/1995           |288     |\n",
      "|22/Jul/1995           |192     |\n",
      "|23/Aug/1995           |345     |\n",
      "|23/Jul/1995           |233     |\n",
      "|24/Aug/1995           |420     |\n",
      "|24/Jul/1995           |328     |\n",
      "|25/Aug/1995           |415     |\n",
      "|25/Jul/1995           |461     |\n",
      "|26/Aug/1995           |366     |\n",
      "|26/Jul/1995           |336     |\n",
      "|27/Aug/1995           |370     |\n",
      "|27/Jul/1995           |336     |\n",
      "|28/Aug/1995           |410     |\n",
      "|28/Jul/1995           |94      |\n",
      "|29/Aug/1995           |420     |\n",
      "|30/Aug/1995           |571     |\n",
      "|31/Aug/1995           |526     |\n",
      "+----------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_results = spark.sql(\"SELECT SUBSTR(data,1,11), count(1) from nasa where retorno = '404' group by SUBSTR(data,1,11) order by 1\")\n",
    "sql_results.show(1000, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 5 - O total de bytes retornandos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+\n",
      "|(((CAST(sum(CAST(bytes AS BIGINT)) AS DOUBLE) / CAST(1024 AS DOUBLE)) / CAST(1024 AS DOUBLE)) / CAST(1024 AS DOUBLE))|\n",
      "+---------------------------------------------------------------------------------------------------------------------+\n",
      "|61.0242736665532                                                                                                     |\n",
      "+---------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_results = spark.sql(\"SELECT sum(bytes)/1024/1024/1024 from nasa\")\n",
    "sql_results.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
